{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code creates the spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Map-Reduce: Gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Create a function `f_linear(b, x)` that receives the parameters `b` and one data point `x` as lists and return the prediction for that data point. Assume that the first element of `b` is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1ee003cc0679b4304ff2fa404558cd6f",
     "grade": false,
     "grade_id": "cell-e260b0944cc2cafb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define below the function `f_linear` which performs a linear prediction based on parameters as data point\n",
    "def f_linear(b, x):\n",
    "    y = b[0]\n",
    "    for i in range(len(x)):\n",
    "        y = y + b[i+1]*x[i]\n",
    "    return(y)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above, if we assume b = [0, 1], and the first data point x = [1], y = 2\n",
    "f_linear([0, 1], [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define the function `L(y_pred, y)` that receives a prediction `y_pred` and the actual value `y` and returns the squared error between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7892a587f9bd66149dc1402292150369",
     "grade": false,
     "grade_id": "cell-b624ab75860c6e6e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def L(y_pred, y):\n",
    "    z = (y - y_pred)**2\n",
    "    return(z)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be not error here\n",
    "L(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a function `gf_linear(f, b, x, y)` which returns the gradient of the linear function `f` with parameter `b` with respect to the squared loss function, evaluated at `x` and the actual outcome `y`. This function should return a vector with each element $j$ corresponding to the gradient with respect $b_j$, with $j = \\{0, 1, \\dots, p\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f7329f34aa05e0b8ef30faedf9750a13",
     "grade": false,
     "grade_id": "cell-2d3493c9766da5c1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define `gf_linear`\n",
    "def gf_linear(f, b, x, y):\n",
    "    f = f_linear(b,x)\n",
    "    gd = []\n",
    "    db_0 = 2*(f-y)\n",
    "    gd.append(db_0)\n",
    "    for i in range(len(x)):\n",
    "        dbi = 2*x[i]*(f-y)\n",
    "        gd.append(dbi)\n",
    "    return(gd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, -2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and first data point\n",
    "x = [1]\n",
    "y = 2\n",
    "b = [0, 1]\n",
    "gf_linear(f_linear, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and second data point\n",
    "x = [-2]\n",
    "y = -1\n",
    "b = [0, 1]\n",
    "f=f_linear(b,x)\n",
    "gf_linear(f, b, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Develop a map-reduce job that produces a value so that the first element of the value is the mean loss function across all the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9783bb5e9503f2ef935c7cb83c2fc43b",
     "grade": false,
     "grade_id": "cell-481c573523c2b098",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "rdd_data = sc.parallelize([\n",
    "    [[1, 2], 3],\n",
    "    [[3, 1], 4],\n",
    "    [[-1, 1.5], 0],\n",
    "    [[-9, 3], 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb5b73b9485caf4962b2cef35a565c10",
     "grade": false,
     "grade_id": "cell-f87dee01323b2e82",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `map_mse` below\n",
    "def map_mse(f, b, L, xy):\n",
    "    f=f_linear(b,xy[0])\n",
    "    L=L(f,xy[1])\n",
    "    return([0,[L,1]])\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should apply the map function in the following way:\n",
    "\n",
    "```python\n",
    "# for an example set of `b = [0, 0, 0]`\n",
    "rdd_data.map(lambda x: map_generator(f_linear, [0, 0, 0], L, x))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [9, 1]], [0, [16, 1]], [0, [0.0, 1]], [0, [0, 1]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a reduce job that receives two values of a previous reduce (or map) and merge them appropriately. Remember that at the end of the reduce job, the first element of the value should be the mean squared error. Create the function `reduce_mse(v1, v2)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4c7b7e49c6fcf2dbf3b3d525ef5e86d",
     "grade": false,
     "grade_id": "cell-664ff0abbe2fe932",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `reduce_mse` below\n",
    "def reduce_mse(v1, v2):\n",
    "    l1,l2 = v1\n",
    "    t1,t2 = v2\n",
    "    rd = [(l1*l2+t1*t2)/(l2+t2),l2+t2]\n",
    "    return(rd)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.8125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 2, 3], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will compute the cumulative gradient of a model on the data. You will define a map function `map_gradient(f, gf, b, xy)` that would receive a function `f`, its gradient `gf`, its parameters `b`, and a data point `xy = [x, y]`. Also you will define a function `reduce_gradient(v1, v2)` that combines the two values appropriately. In the map function, you probably do not need to keep extra values beyond the actual gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b5464a0f7f432697ee6d51651071c13",
     "grade": false,
     "grade_id": "cell-f6fa0087a0af6012",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `map_gradient` below\n",
    "def map_gradient(f, gf, b, xy):\n",
    "    gf = gf_linear(f,b,xy[0],xy[1])\n",
    "    return([0,gf])\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba1a1e364bf1c1610dba927c27ce4ca8",
     "grade": false,
     "grade_id": "cell-d3a5661b89fd6eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `reduce_gradient` below\n",
    "def reduce_gradient(v1, v2):\n",
    "    rg=[]\n",
    "    for i in range(len(v1)):\n",
    "        rg.append(v1[i]+v2[i])\n",
    "    return(rg)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-14.0, -30.0, -20.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we can run an optimization below, and the MSE should decrease with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.7000000000000001, 1.5, 1.0]\n",
      "Current MSE: \t\t 25.702500000000004\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [1.2700000000000002, -8.03, 3.375]\n",
      "Current MSE: \t\t 1941.2104390624995\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-5.887249999999999, 76.03925, -20.932375]\n",
      "Current MSE: \t\t 154868.55739933692\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [58.490481249999995, -674.7452812500001, 197.19040937500003]\n",
      "Current MSE: \t\t 12361757.656351853\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-516.9456870312499, 6032.90305703125, -1751.7632777343752]\n",
      "Current MSE: \t\t 986733680.53671\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [4624.096880300781, -53895.11618255079, 15660.883497880859]\n",
      "Current MSE: \t\t 78762540125.70647\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-41307.57420476065, 481518.98704511696, -139908.6478753955]\n",
      "Current MSE: \t\t 6286942318447.336\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [369059.0336107604, -4302024.060799995, 1249994.2051317175]\n",
      "Current MSE: \t\t 501833024339880.2\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-3297273.9701623293, 38435520.600785956, -11167796.19629538]\n",
      "Current MSE: \t\t 4.0057053423124936e+16\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [29458795.825595718, -343393952.8698364, 99776302.51215337]\n",
      "Current MSE: \t\t 3.1974131854988303e+18\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.05\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc837820a5a204f6d39c8fd064139e4b",
     "grade": true,
     "grade_id": "cell-49ee8798a623ecdf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.7000000000000001, 1.5, 1.0]\n",
      "Current MSE: \t\t 25.702500000000004\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [1.2700000000000002, -8.03, 3.375]\n",
      "Current MSE: \t\t 1941.2104390624995\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-5.887249999999999, 76.03925, -20.932375]\n",
      "Current MSE: \t\t 154868.55739933692\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [58.490481249999995, -674.7452812500001, 197.19040937500003]\n",
      "Current MSE: \t\t 12361757.656351853\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-516.9456870312499, 6032.90305703125, -1751.7632777343752]\n",
      "Current MSE: \t\t 986733680.53671\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [4624.096880300781, -53895.11618255079, 15660.883497880859]\n",
      "Current MSE: \t\t 78762540125.70647\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-41307.57420476065, 481518.98704511696, -139908.6478753955]\n",
      "Current MSE: \t\t 6286942318447.336\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [369059.0336107604, -4302024.060799995, 1249994.2051317175]\n",
      "Current MSE: \t\t 501833024339880.2\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-3297273.9701623293, 38435520.600785956, -11167796.19629538]\n",
      "Current MSE: \t\t 4.0057053423124936e+16\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [29458795.825595718, -343393952.8698364, 99776302.51215337]\n",
      "Current MSE: \t\t 3.1974131854988303e+18\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.05\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate determines how slowly or fast we will move towards the optimal solution. If it is too small we may need many iterations to find optimal value. If it is too large, wemay skip optimal value. For value of 0.05, the value increases till a point and then decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
